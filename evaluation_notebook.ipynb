{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-27T16:39:53.256367Z",
     "start_time": "2023-06-27T16:39:25.704471Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import sys\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "\n",
    "sys.path.append('/gpfs/space/home/joonas97/MIL/AttentionDeepMIL')\n",
    "from dataloader_TUH import TUH_full_scan_dataset\n",
    "from model import Attention, GatedAttention, ResNet18Attention\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "sys.path.append('/gpfs/space/home/joonas97/KITSCAM/')\n",
    "from utils.visualize_utils import *\n",
    "\n",
    "from IPython.display import HTML\n",
    "import matplotlib\n",
    "import glob\n",
    "import nibabel as nib\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.express as px\n",
    "import re\n",
    "matplotlib.rcParams['animation.embed_limit'] = 2 ** 128\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import nibabel as nib\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import transforms\n",
    "from monai.transforms import CropForeground, CenterSpatialCrop\n",
    "import torch.nn.functional as F\n",
    "import os\n",
    "\n",
    "\n",
    "def threshold_at_one(x):\n",
    "    # threshold at 1\n",
    "    return x > -1\n",
    "\n",
    "\n",
    "background_cropper = CropForeground(select_fn=threshold_at_one)\n",
    "center_cropper = CenterSpatialCrop(roi_size=(512, 512, 1000))  #500\n",
    "\n",
    "\n",
    "class TUH_full_scan_dataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, dataset_type, only_every_nth_slice=1, interpolation=False, augmentations=None, as_rgb=False,\n",
    "                 min_max_normalization=False):\n",
    "        super(TUH_full_scan_dataset, self).__init__()\n",
    "        self.as_rgb = as_rgb\n",
    "        self.min_max_norm = min_max_normalization\n",
    "        self.augmentations = augmentations\n",
    "        self.nth_slice = only_every_nth_slice\n",
    "        self.interpolation = interpolation\n",
    "        data_path = '/gpfs/space/home/joonas97/nnUNet/nn_pipeline/nnUNet_preprocessed/Task602_tuh_final/nnUNetData_plans_v2.1_stage1/'\n",
    "        data_path = '/gpfs/space/projects/BetterMedicine/joonas/tuh_kidney_study/new_setup/MIL_EXP/'\n",
    "\n",
    "        if dataset_type == \"train\":\n",
    "            data_path = os.path.join(data_path, \"train\")\n",
    "        elif dataset_type == \"test\":\n",
    "            data_path = os.path.join(data_path, \"test\")\n",
    "        else:\n",
    "            raise ValueError(\"Dataset type should be either train or test\")\n",
    "        control_path = data_path + '/controls2/*nii.gz'\n",
    "        tumor_path = data_path + '/tumors2/*nii.gz'\n",
    "        print(\"PATHS: \")\n",
    "        print(control_path)\n",
    "        print(tumor_path)\n",
    "        control = glob.glob(control_path)\n",
    "        tumor = glob.glob(tumor_path)\n",
    "\n",
    "        control_labels = [[False]] * len(control)\n",
    "        tumor_labels = [[True]] * len(tumor)\n",
    "\n",
    "        self.img_paths = control + tumor\n",
    "        self.labels = control_labels + tumor_labels\n",
    "\n",
    "        print(\"Data length: \", len(self.img_paths), \"Label length: \", len(self.labels))\n",
    "        print(\n",
    "            f\"control: {len(control)}, tumor: {len(tumor)}\")\n",
    "\n",
    "        self.classes = [\"control\", \"tumor\"]\n",
    "\n",
    "    def __len__(self):\n",
    "        # a DataSet must know its size\n",
    "        return len(self.img_paths)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        path = self.img_paths[index]\n",
    "\n",
    "        x = nib.load(path).get_fdata()\n",
    "        x = x[:, :, ::self.nth_slice]\n",
    "        clipped_x = np.clip(x, np.percentile(x, q=0.05), np.percentile(x, q=99.5))\n",
    "        norm_x = (clipped_x - np.mean(clipped_x, axis=(0, 1))) / np.std(clipped_x, axis=(0, 1))  # mean 0, std 1 norm\n",
    "        #norm_x = (clipped_x - np.min(clipped_x)) / (np.max(clipped_x) - np.min(clipped_x)) ## 0-1 norm\n",
    "\n",
    "        norm_x = torch.unsqueeze(torch.from_numpy(norm_x), 0)\n",
    "        norm_x = center_cropper(norm_x)\n",
    "\n",
    "        _, h, w, d = norm_x.shape\n",
    "        if self.interpolation:\n",
    "            norm_x = F.interpolate(torch.unsqueeze(norm_x, 0), size=(int(h / 2), int(w / 2), d),\n",
    "                                   mode='trilinear', align_corners=False)\n",
    "\n",
    "        norm_x = torch.squeeze(norm_x)\n",
    "\n",
    "        x = norm_x.to(torch.float16)\n",
    "        y = torch.tensor(self.labels[index])\n",
    "\n",
    "        if self.as_rgb:\n",
    "            x = torch.stack([x, x, x], dim=0)\n",
    "            x = torch.squeeze(x)\n",
    "\n",
    "        return x, y, self.img_paths[index]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-27T16:39:53.569136Z",
     "start_time": "2023-06-27T16:39:53.248595Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PATHS: \n",
      "/gpfs/space/projects/BetterMedicine/joonas/tuh_kidney_study/new_setup/MIL_EXP/test/controls2/*nii.gz\n",
      "/gpfs/space/projects/BetterMedicine/joonas/tuh_kidney_study/new_setup/MIL_EXP/test/tumors2/*nii.gz\n",
      "Data length:  78 Label length:  78\n",
      "control: 39, tumor: 39\n"
     ]
    }
   ],
   "source": [
    "nth_slice = 4\n",
    "test_dataset = TUH_full_scan_dataset(dataset_type=\"test\", only_every_nth_slice=nth_slice,\n",
    "                                     interpolation=False, as_rgb=True)\n",
    "data_loader = torch.utils.data.DataLoader(test_dataset, batch_size=1, shuffle=True, num_workers=2,\n",
    "                                          pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-27T16:39:56.290515Z",
     "start_time": "2023-06-27T16:39:53.306977Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model loaded!\n"
     ]
    }
   ],
   "source": [
    "model = ResNet18Attention()\n",
    "model.load_state_dict(torch.load(\n",
    "    '/gpfs/space/home/joonas97/MIL/AttentionDeepMIL/results/main_joonas/2023-06-26/19-24-49/checkpoints/best_model.pth'))\n",
    "model.cuda()\n",
    "print(\"model loaded!\")\n",
    "\n",
    "# get ground truth segmentations\n",
    "ctrls = glob.glob('/gpfs/space/projects/BetterMedicine/joonas/tuh_kidney_study/new_setup/controls2/labelsTr/*nii.gz')\n",
    "tmrs = glob.glob('/gpfs/space/projects/BetterMedicine/joonas/tuh_kidney_study/new_setup/tumors2/labelsTr/*nii.gz')\n",
    "all_labels = ctrls + tmrs\n",
    "all_labels[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-27T16:38:28.238861Z",
     "start_time": "2023-06-27T16:38:22.213984Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "start_string = 'case_'\n",
    "end_string = '_0000'\n",
    "step = 0\n",
    "for data, bag_label, path in data_loader:\n",
    "    step +=1\n",
    "    case_id = re.search(start_string + '(.*)' + end_string, path[0]).group(1)\n",
    "    matching_gt = [s for s in all_labels if case_id in s][0]\n",
    "    seg = nib.load(matching_gt).get_fdata()[:, :, ::nth_slice]\n",
    "    slice_data = [np.unique(seg[:, :, i], return_counts=True) for i in range(seg.shape[2])]\n",
    "\n",
    "    df = pd.DataFrame(slice_data, columns=[\"label\", \"count\"])\n",
    "    \n",
    "    df = df.explode([\"label\", \"count\"])\n",
    "    df = pd.concat([df, pd.DataFrame({\"label\": [2.0], \"count\": [0]})])\n",
    "    \n",
    "    df = pd.pivot_table(df, index=df.index, columns=\"label\", values=\"count\", fill_value=0)\n",
    "    df = df.rename(columns={0.0: \"none\", 1.0: \"kidney\", 2.0: \"tumor\"})\n",
    "\n",
    "    _, c, x, y, h = data.shape\n",
    "    data = torch.reshape(data, (1, h, c, x, y))\n",
    "    model.train()\n",
    "    data, bag_label = data.cuda(), bag_label.cuda()\n",
    "    with torch.no_grad(), torch.cuda.amp.autocast():\n",
    "        Y_prob, predicted_label, attention_weights = model.forward(data)\n",
    "    print(\"predicted label: \",predicted_label)\n",
    "    print(\"attenttion length: \", len(attention_weights.cpu().as_tensor()[0]))\n",
    "    \n",
    "    print(\"dataframe length:\", len(df))\n",
    "    print(\"data shape: \", data.shape)\n",
    "    print(\"seg shape: \", seg.shape)\n",
    "    df[\"attention\"] = pd.Series(attention_weights.cpu().as_tensor()[0]) * 30000\n",
    "    fig = px.bar(df, x=df.index, y=[\"kidney\", \"attention\", \"tumor\"], barmode=\"overlay\",\n",
    "                 title=str(path[0].split(\"/\")[9:]))\n",
    "    fig.show()\n",
    "    print(step)\n",
    "    if step > 15:\n",
    "        break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
